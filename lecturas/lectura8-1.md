# Personal opinion on:

### Pu, P., Chen, L. and Hu, R. (2011). A user-centric evaluation framework for recommender systems. RecSys'11 - Proceedings of the 5th ACM Conference on Recommender Systems. 157-164.

In this paper, the authors explore into a different way of evaluating recommender systems. The traditional way is to compare different systems by their accuracy on recommendations. This is not a bad metric, moreover, without a baseline level of accuracy, any other way of evaluating recommendations is probably pointless, but, past a certain point of accuracy, increasing it's value starts to lose it's purpose. In fact, we could consider that at certain point, too much accuracy turns the system into a predictor of what the user will interact with or buy, and not a recommender for possible novel items the user might be interested in.

Because of that, there are several ways of evaluating a recommender agent from the user point of view. In this topic, the authors developed _ResQue_ (Recommender system's Quality of user experience). For this, they based their research on some previous frameworks, specifically TAM (Technology Acceptance Model), to develop a _survey_ for the user, in which the owners of a recommender system can obtain direct feedback from the users, to see if their system is providing accurate and satisfying recommendations or not.

The structure of the questions is not linear, but rather a multi-layer structure based on higher-level constructs. Each of this higher-level constructs have different objectives, and are, also, split among various sub-sections. The higher-level constructs are: 1) _Perceived system qualities_, which aims to evaluate certain objective characteristics of the system, such as the quality of the recommendations, or the adequacy of the interface. 2) _Beliefs_, which aims to how the user perceives the system, mainly how effective they feel the system is at helping them. 3) _Attitudes_, which is related to the feeling of a user towards the system, differentiating from beliefs as this is more long-lasting, as an user might consider the system actually helps them, but might not like using it. 4) _Behavioral intentions_, which measures if the users actually believe in the system, or if they are actually taking into consideration the results given by it.

Then, the authors move on to explaining the process for validating their model. For this, they used several well-knows recommender systems, and the users were mostly college students from a wide-variety of countries, providing a good homogeneous population. Afterwards, they describe their experimental setup, along displaying several of their metrics used for this. Personally, I felt lost in this section. As someone without a real background on more psychological oriented models, a little more explanation onto why some of these metrics were considered, or what they actually mean of portray, would've been fantastic. Finally, there is a table which contains the whole model with it's 32 questions, and the authors also mention that, if a shorter version is needed, one question from each construct should be enough for the assessment of that construct. 

In my opinion, this line of work is really good for evaluating recommender systems, when the circumstances are such for a system like this to be applied. As mentioned in the beginning, traditional accuracy metrics are not bad, but they are not really meaningful if the users dislike the results of the system. In this line of thinking, this kind of evaluation for systems is always spot on in what the user expects of the system. This means that, users are the ones that indicate if they want a system that is super accurate, or a one that recommends items that they would never have found, but that only recommends items they are interested in every once in a while, and that cannot be captured in traditional accuracy metrics.