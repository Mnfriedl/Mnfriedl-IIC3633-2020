# Personal opinion on:

### Ca√±amares, R., Redondo, M., & Castells, P. (2019). Multi-armed recommender system bandit ensembles. In Proceedings of the 13th ACM Conference on Recommender Systems (pp. 432-436).

In this paper, the authors explore into the idea of using multi-armed bandits for the purpose of making ensembles of different recommendation algorithms, turning the ensemble into a cyclic process where it can observe and learn about each algorithm. An ensemble is a combination of various recommendation algorithms, where, depending on the circumstances or the desired output, one or multiple algorithms are selected to produce the desired output at a certain point. The main challenge in this is how to select the algorithm each time, or which algorithms to consider and then how to combine their respective outputs to generate a final list of recommendations.

As stated in the introduction, a big problem in traditional ensembles is that as runtime conditions evolve, the performance of the different algorithms included in the ensemble may vary. To tackle this problem, two bandit algorithms were adapted for use with recommendation algorithms, thompson sampling and <img src="https://render.githubusercontent.com/render/math?math=\epsilon\text{-greedy}">. For this, they considered that each arm of the bandit is a different recommendation algorithm. This means that when a particular arm is selected, the algorithm associated to this arm is run to select items to recommend. The rewards are 1 if the user is pleased by it, and zero otherwise.

for the experiments, the authors chose to run kNN, matrix factorization and most-popular algorithms. According to the paper, they chose just 3 algorithms to be able to examine the behavior of the bandit. Despite this being a good approach, as we want to be able to see what the bandit is doing, and draw conclusions from it, another, bigger experiment would've been nice, like a bandit with more arms (more algorithms to choose from), and a wider variety of algorithms.

In the results we can see that the bandits indeed outperformed the pure algorithms. Also, <img src="https://render.githubusercontent.com/render/math?math=\epsilon\text{-greedy}"> seems to have the best performace in this test. Then, in figure 2, we can see the fraction of users that were recommended by each arm (algorithm) in each epoch by both bandits. Both graphs look very similar, where most popular is mostly chosen for the first 30 or so epochs, and then matrix factorization prevails over the other arms. In the <img src="https://render.githubusercontent.com/render/math?math=\epsilon\text{-greedy}"> graph, we can see how the fraction of recommendations transitions from mostly most-popular, to mostly matrix factorization in a single epoch. Also, we can see that the non-prevalent arms both share a similar fraction of the total recommendations.

I feel like this harsh evolution of the <img src="https://render.githubusercontent.com/render/math?math=\epsilon\text{-greedy}"> bandit, and way too constant fractions may be detrimental to the performance in bigger experiments. The thompson sampling bandit clearly has preferred algorithms over time too, but the change in them is slower, and doesn't feel so overfitted (maybe this is not the correct term, but I'm trying to say that <img src="https://render.githubusercontent.com/render/math?math=\epsilon\text{-greedy}"> feels way too scheduled and all-in in it's variations). As mentioned before, an experiment with more arms would be interesting, and see if <img src="https://render.githubusercontent.com/render/math?math=\epsilon\text{-greedy}"> still outperforms the thompson bandit, and if the fractions still look the same.