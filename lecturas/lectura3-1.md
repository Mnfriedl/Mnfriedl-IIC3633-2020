# Personal opinion on:

### Cremonesi, P., Koren, Y., & Turrin, R. (2010). Performance of recommender algorithms on top-n recommendation tasks. In Proceedings of the fourth ACM conference on Recommender systems (pp. 39-46). ACM.

The focus of this paper, is to explore into new and better ways to quantify the performance of recommender systems. The authors of the paper start off by mentioning the most common way of performing such evaluations, which is RMSE. RMSE means root mean square error, which aims to minimize the error between predicted ratings for possible recommendations, and actual rating given by users. The problem with this is, that for most systems, the best recommendations are shown, but not it's predicted ratings. This means that, while the system is being optimized to predict a certain rating, it is not really used for this purpose, but for another.

Following this line, the authors then propose their own, novel way of measuring the performance of recommender systems. The first interesting approach they take is how they chose to split the dataset for train and test. The test sets, denoted as <img src="https://render.githubusercontent.com/render/math?math=T">, only contains 5-stars ratings. This is so the test set only contains items that are relevant to the users. Traditionally, the test sets used include all kind of reviews, positive and negative. By using only extremely positive reviews, we are only focusing the testing process on the fact that the system can actually recommend items that the user is going to like, and we are leaving behind the negative reviews, which don't really help when trying to recommend items that the user will actually like.

Then, they defined the metrics that were going to be used to evaluate the systems, _recall_ and _precision_, which are pretty standard in the machine learning world. After this is the second interesting approach, how they further partitioned the test sets, considering how certain items are easier to recommend than others, because the sheer amount of reviews they have in comparison to the rest of the dataset is too big. This is knows as the _long-tail_ distribution, and they chose to partition the test datasets into <img src="https://render.githubusercontent.com/render/math?math=T_{head}"> and <img src="https://render.githubusercontent.com/render/math?math=T_{tail}">, where the first contains the items that hold 33% of the ratings, and the later, the rest of the items. With this, they want to be able to measure how every system performs on less known items, which are harder to recommend.

After this they move on to explaining the most common systems at the moment, and which they are going to test with this new metrics. We can then see the results of 8 different systems, and how they perform according to the proposed metrics. In this sections, there are several interesting things to be considered. The first, would be how on the <img src="https://render.githubusercontent.com/render/math?math=T_{tail}"> test set, the matrix factorization (SVD) approaches seem to outperform kNN when the ammount of reviews isn't that large (Movielens dataset). We can also see how the PureSVD system performed better in more popular items with less latent factors, but it performed better on less knows items with more latent factors. This opens up a possible discussion about the best amount of latent factors to be used in a system. Where lays the equilibrium of the system, if there is any, between popular and un-popular item recommendations, or if it's worth sacrificing the performance in more popular items to get better less-knows items recommendations.