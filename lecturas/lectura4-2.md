# Personal opinion on:

### Document clustering based on non-negative matrix factorization. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval (pp. 267-273). ACM.

The paper focuses on a novel way to categorize documents into clusters. They begin with an introduction to the subject as to why this is relevant, and a little explanation on the different approaches that have been explored in the past for the subject, which are document partitioning, and agglomerative clustering. Right after, they move onto the related work, where they focus mainly in three techniques: LSI, SVD and spectral clustering. All three methods have something in common, they aim to map each document to a latent space of features, and the use this information to perform clustering to categorize the items.

With this settled, then they propose their own method. The core idea of the method is pretty similar, they aim to map the items in the corpus into a latent space of features. The main difference here is, each of this latent features is one cluster, which means they no longer need to use another algorithm (like kNN) after to perform the clustering. So, if we consider that each possible topic in our corpus is a feature, the process of mapping the items to this latent space, makes the clustering unnecessary, as we can just take <img src="https://render.githubusercontent.com/render/math?math=arg max v_{ij}">, and assign the item to that specific cluster. This means that, for a specific item, it's gonna have one value for each possible latent feature, which are also our clusters, so we just look up which is the latent feature with the biggest value, and assign this item to that particular cluster.

So long, this is pretty interesting, as we are getting rid of the necessity to run a clustering algorithm after our matrix factorization method, but, there is more, which NMF even more interesting. In all the other algorithms mentioned, the latent features obtained from running the algorithms are all orthogonal, so, it is similar to running some data reduction algorithm, like PCA. With this, you are assuming that every feature is totally independent from every other feature available in the latent space, but this is not necessarily true.

I struggled a bit visualizing the last statement, but after thinking of it in a different matter, it became quite easier. Let's say we have a document corpus, but, each document is a song instead of some text. We want to categorize the songs into their respective genre, so, each of our latent features is going to be one of all the music genres there is. Are all music genres totally independent of each other? well, no. Let's say we have two songs, which must go to different clusters, one is part of the Heavy Metal genre and the other is part of the Thrash Metal genre. It is to expect that both these genres are, actually, pretty similar, so having them to be orthogonal in our final space just doesn't make sense.

To wrap things up, they introduced two datasets, or document corporas. In both datasets they chose to remove the documents that belonged to clusters with less than 5 total documents. Although it would be interesting to see how the algorithm performed with this clusters in, it makes sense, as clusters that are too little, like a cluster that had only 1 document in the TDT2 dataset, is just an outlier to the data, and makes noise. Then, the performace metrics are explained, and a table with results is shown. Here, we can see how NMF-NCW outperformed every other algorithm consistently, through both datasets. NMF-NCW is a variant of the NMF algorithm proposed, which takes the weighting method from the NC weighted variation and implements it in the NMF algorithm. It is no surprise that the method yields better results than the others, as they are pretty similar in it's core idea, but, with some improvements to the process, as explained previously.