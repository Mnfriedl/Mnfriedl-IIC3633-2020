# Personal opinion on:

### Bendada, W., Salha, G., & Bontempelli, T. (2020). Carousel Personalization in Music Streaming Apps with Contextual Bandits. In Fourteenth ACM Conference on Recommender Systems (pp. 420-425).

In this paper, the authors utilize multi-armed bandits for the task of recommending items, specifically music playlists, to place on carousels for users to interact. What is interesting here is that they are using the bandit as their recommendation algorithm, and are not using other traditional algorithms to generate the recommendation the user is going to receive. First, they explain how a multi-armed bandit works, and how they plan to use them for the task in mind. Right after, they propose two ways to reduce the amount of parameters needed for the system. This is because the naive approach would be too large, and not practical in real applications.

For their first approach on reducing the parameters, they propose a semi-personalized way. For this, the users need to be clustered beforehand, using a clustering algorithm of preference. After this, for each cluster there has to be a bandit, that will generate the recommendations. All the users within a single cluster get the same recommendations. The second way is to have a D-dimensional vector which represents the users preferences of their musical taste. Then, the system learns one vector for each item. This approach feels very similar to a matrix factorization recommender algorithm, with the difference that to generate the recommendations, we are using different types of bandits instead.

Then, the authors explain their proposed way to deal with carousels in real-world applications. The premise of this is: carousel items are not always visible, as users need to scroll to reach them, and therefor, items are not necessarily seen by users just because they were in the carousel. To counter this problem, they propose to have a reward system where a recommendation can be labeled with a 1 (successful, the user interacted with the item), 0 (unsuccessful) or an X, meaning the user didn't see the recommendation. This part is really interesting, as bandits work in a cyclic manner, learning over time, and the fact that items on a carousel aren't always seen may cause to have negative feedback on items that might have been enjoyed by the user, given they were displayed earlier in the carousel.

After this, the different bandit algorithms used are displayed an explained, and then they move to the experiments. Two experiments were performed: an offline and an online experiment. The offline experiment offers a wider testing of the algorithms, as the online experiment was performed on a real app with real users, and thus only the best performing algorithm from the offline experiments was selected. In the results multiple things can be seen: semi-personalized approach performed better, that their _cascade_ models outperformed the _no-cascade_ alternatives, and that the bandits showed good performances across the table. Also, pessimistic algorithms seem to outperform their non-pessimistic alternatives.

The problem with this results is: the system is considering any interaction as a successful recommendation. As an engagement method, this is not bad, as you are getting the user to interact with your product, and make it visible to him. But, in this context, we are dealing with music recommendations. For this system, if a user clicks on a playlists and listens to the first 5 seconds of a song is as successful as if a user clicks on a playlist and spends three hours listening the playlist to its end. An extended experiment, where users approval of the recommendations (by listening time, or maybe a survey after using the system) is recorded would be interesting.